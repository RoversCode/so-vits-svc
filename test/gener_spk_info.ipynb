{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_dir_files(root_dir, ext=\"dic\"):\n",
    "    paths_list = []\n",
    "    for parent, _, fileNames in os.walk(root_dir):\n",
    "        for name in fileNames:\n",
    "            if name.startswith(\".\"):  # 去除隐藏文件\n",
    "                continue\n",
    "            if ext:  # 根据后缀名搜索\n",
    "                if name.endswith(tuple(ext)):\n",
    "                    # names_list.append(name)\n",
    "                    paths_list.append(os.path.join(parent, name))\n",
    "            else:\n",
    "                # names_list.append(name)\n",
    "                paths_list.append(os.path.join(parent, name))\n",
    "\n",
    "    return paths_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用模型进一步构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# np.load(all_files[0])\n",
    "\n",
    "\n",
    "def cosine_similarity(arr1, arr2):\n",
    "    # 确保输入的数组形状为[1, 192]\n",
    "\n",
    "    assert arr1.shape == (1, 192), \"arr1的形状应为[1, 192]\"\n",
    "    assert arr2.shape == (1, 192), \"arr2的形状应为[1, 192]\"\n",
    "\n",
    "    # 计算数组的二范数(L2范数)\n",
    "    norm1 = np.linalg.norm(arr1)\n",
    "    norm2 = np.linalg.norm(arr2)\n",
    "\n",
    "    # 计算数组的点积\n",
    "    dot_product = np.dot(arr1, arr2.T)\n",
    "\n",
    "    # 计算余弦相似度\n",
    "    cosine_sim = dot_product / (norm1 * norm2)\n",
    "\n",
    "    return cosine_sim[0][0]\n",
    "\n",
    "\n",
    "def cosine_similarity_batch(arr1, arr2):\n",
    "    # 确保输入的数组形状为[batch_size, feature_size]\n",
    "    if arr1.shape != arr2.shape:\n",
    "        # arr1的最后一个维度等于arr2的最后一个维度\n",
    "        arr1 = arr1[: arr2.shape[0], :]\n",
    "\n",
    "    # 计算数组的二范数(L2范数)\n",
    "    norm1 = np.linalg.norm(arr1, axis=1, keepdims=True)\n",
    "    norm2 = np.linalg.norm(arr2, axis=1, keepdims=True)\n",
    "\n",
    "    # 计算数组的点积\n",
    "    dot_product = np.sum(arr1 * arr2, axis=1, keepdims=True)\n",
    "\n",
    "    # 计算余弦相似度\n",
    "    cosine_sim = dot_product / (norm1 * norm2)\n",
    "\n",
    "    return cosine_sim.squeeze()\n",
    "\n",
    "\n",
    "def cosine_similarity_gpu(tensor1, tensor2, epsilon=1e-8):\n",
    "    # 确保输入的张量形状为[1, 192]\n",
    "    assert tensor1.shape == (1, 192), \"tensor1的形状应为[1, 192]\"\n",
    "    assert tensor2.shape == (1, 192), \"tensor2的形状应为[1, 192]\"\n",
    "\n",
    "    # 将张量移动到GPU上\n",
    "    tensor1 = tensor1.cuda()\n",
    "    tensor2 = tensor2.cuda()\n",
    "\n",
    "    # 计算张量的二范数(L2范数)\n",
    "    norm1 = torch.norm(tensor1)\n",
    "    norm2 = torch.norm(tensor2)\n",
    "    # 对分母进行平滑处理,防止除以零\n",
    "    denominator = torch.clamp(norm1 * norm2, min=epsilon)\n",
    "    # 计算张量的点积\n",
    "    dot_product = torch.dot(tensor1.view(-1), tensor2.view(-1))\n",
    "\n",
    "    # 计算余弦相似度\n",
    "    cosine_sim = dot_product / denominator\n",
    "\n",
    "    return cosine_sim.item()\n",
    "\n",
    "\n",
    "def cosine_similarity_batch_torch(tensor1, tensor2, epsilon=1e-8):\n",
    "    # 确保输入的张量形状为[batch_size, feature_size]\n",
    "    assert tensor1.shape == tensor2.shape, \"tensor1和tensor2的形状应该相同\"\n",
    "\n",
    "    # 计算张量的二范数(L2范数)\n",
    "    norm1 = torch.norm(tensor1, dim=1, keepdim=True)\n",
    "    norm2 = torch.norm(tensor2, dim=1, keepdim=True)\n",
    "\n",
    "    # 对分母进行平滑处理,防止除以零\n",
    "    denominator = torch.clamp(norm1 * norm2, min=epsilon)\n",
    "\n",
    "    # 计算张量的点积\n",
    "    dot_product = torch.sum(tensor1 * tensor2, dim=1, keepdim=True)\n",
    "\n",
    "    # 计算余弦相似度\n",
    "    cosine_sim = dot_product / denominator\n",
    "\n",
    "    return cosine_sim.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = traverse_dir_files(\"/mnt/e/Workspace/growth/audio/so-vits-svc/preprocess/raw_data/蜡笔小新切片\",'wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:04<00:00, 18.29it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.24it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.80it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.92it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.60it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 17.03it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 17.85it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 17.53it/s]\n",
      "  6%|▋         | 5/78 [00:00<00:05, 13.88it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sub_file), batch_size)):\n\u001b[1;32m     27\u001b[0m     batch_files \u001b[38;5;241m=\u001b[39m sub_file[j : j \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m---> 28\u001b[0m     spk2s \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mload(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m batch_files]\n\u001b[1;32m     29\u001b[0m     spk2s \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(np\u001b[38;5;241m.\u001b[39mstack(spk2s, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# x = cosine_similarity_batch(spk1, spk2s)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[53], line 28\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sub_file), batch_size)):\n\u001b[1;32m     27\u001b[0m     batch_files \u001b[38;5;241m=\u001b[39m sub_file[j : j \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m---> 28\u001b[0m     spk2s \u001b[38;5;241m=\u001b[39m [\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m batch_files]\n\u001b[1;32m     29\u001b[0m     spk2s \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(np\u001b[38;5;241m.\u001b[39mstack(spk2s, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# x = cosine_similarity_batch(spk1, spk2s)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/amphion/lib/python3.9/site-packages/numpy/lib/npyio.py:413\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode)\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 413\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m~/.conda/envs/amphion/lib/python3.9/site-packages/numpy/lib/format.py:731\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    729\u001b[0m version \u001b[38;5;241m=\u001b[39m read_magic(fp)\n\u001b[1;32m    730\u001b[0m _check_version(version)\n\u001b[0;32m--> 731\u001b[0m shape, fortran_order, dtype \u001b[38;5;241m=\u001b[39m \u001b[43m_read_array_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    733\u001b[0m     count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/amphion/lib/python3.9/site-packages/numpy/lib/format.py:594\u001b[0m, in \u001b[0;36m_read_array_header\u001b[0;34m(fp, version)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# The header is a pretty-printed string representation of a literal\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;66;03m# Python dictionary with trailing newlines padded to a ARRAY_ALIGN byte\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;66;03m# boundary. The keys are strings.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# Versions (2, 0) and (1, 0) could have been created by a Python 2\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# implementation before header filtering was implemented.\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 594\u001b[0m     header \u001b[38;5;241m=\u001b[39m \u001b[43m_filter_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    596\u001b[0m     d \u001b[38;5;241m=\u001b[39m safe_eval(header)\n",
      "File \u001b[0;32m~/.conda/envs/amphion/lib/python3.9/site-packages/numpy/lib/format.py:555\u001b[0m, in \u001b[0;36m_filter_header\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    553\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    554\u001b[0m last_token_was_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 555\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenize\u001b[38;5;241m.\u001b[39mgenerate_tokens(StringIO(s)\u001b[38;5;241m.\u001b[39mreadline):\n\u001b[1;32m    556\u001b[0m     token_type \u001b[38;5;241m=\u001b[39m token[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    557\u001b[0m     token_string \u001b[38;5;241m=\u001b[39m token[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/amphion/lib/python3.9/tokenize.py:535\u001b[0m, in \u001b[0;36m_tokenize\u001b[0;34m(readline, encoding)\u001b[0m\n\u001b[1;32m    531\u001b[0m token, initial \u001b[38;5;241m=\u001b[39m line[start:end], line[start]\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (initial \u001b[38;5;129;01min\u001b[39;00m numchars \u001b[38;5;129;01mor\u001b[39;00m                 \u001b[38;5;66;03m# ordinary number\u001b[39;00m\n\u001b[1;32m    534\u001b[0m     (initial \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mTokenInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNUMBER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m initial \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parenlev \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, type, string, start, end, line)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "spk_info = {}\n",
    "processed = {}\n",
    "file_paths = file_paths[:10000]\n",
    "for i in range(len(file_paths)):\n",
    "    # 检查i是否已经被归类\n",
    "    spk_name = all_files[i].split(\"/\")[-1].replace(\".npy\", \"\")\n",
    "    if spk_name in processed:\n",
    "        continue\n",
    "    # 新建一个spk_name\n",
    "    if spk_name not in spk_info:\n",
    "        spk_info[spk_name] = [spk_name]\n",
    "    else:\n",
    "        assert False, \"spk_name已经存在，程序存在逻辑错误\"\n",
    "    sub_file = all_files[i + 1 :]\n",
    "    # 剔除已经被归类的音频\n",
    "    # 分批次\n",
    "    batch_size = 128\n",
    "    spk1 = np.load(all_files[i])\n",
    "    # numpy在第一个维度复制\n",
    "    spk1 = np.repeat(spk1, batch_size, axis=0)\n",
    "    for j in tqdm(range(i + 1, len(sub_file), batch_size)):\n",
    "        batch_files = sub_file[j : j + batch_size]\n",
    "        spk2s = [np.load(p) for p in batch_files]\n",
    "        spk2s = np.squeeze(np.stack(spk2s, axis=0))\n",
    "        # x = cosine_similarity_batch(spk1, spk2s)\n",
    "        x = cosine_similarity_batch(spk1, spk2s)\n",
    "        indexs = np.where(x > 0.75)[0]\n",
    "        for index in indexs:\n",
    "            # 记录文件名\n",
    "            t_name = batch_files[index].split(\"/\")[-1].replace(\".npy\", \"\")\n",
    "            spk_info[spk_name].append(t_name)\n",
    "            processed[t_name] = True\n",
    "    # 保存Spk_info和processed\n",
    "    json.dump(spk_info, open(\"spk_info.json\", \"w\"), ensure_ascii=False, indent=4)\n",
    "    json.dump(processed, open(\"processed.json\", \"w\"), ensure_ascii=False, indent=4)\n",
    "\n",
    "    # for j in tqdm(range(i + 1, len(all_files))):\n",
    "    #     spk1 = np.load(all_files[i])\n",
    "    #     spk2 = np.load(all_files[j])\n",
    "    #     if cosine_similarity_gpu(torch.from_numpy(spk1), torch.from_numpy(spk2)) > 0.9:\n",
    "    #         print(all_files[i], all_files[j])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amphion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
